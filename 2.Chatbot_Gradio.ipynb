{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debd70d5",
   "metadata": {},
   "source": [
    "# ChatGPT with custom knowledge about World Cup 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f451b",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b51416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
    "from langchain import OpenAI\n",
    "import gradio as gr\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c9edf",
   "metadata": {},
   "source": [
    "### Authentication - API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05e33b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = 'INSERT YOUR API KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455bde40",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20d09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_index(directory_path):\n",
    "    \n",
    "    # Set maximum input size\n",
    "    max_input_size = 4096\n",
    "    \n",
    "    # Set number of output tokens\n",
    "    num_outputs = 500\n",
    "    \n",
    "    # Set maximum chunk overlap\n",
    "    max_chunk_overlap = 20\n",
    "    \n",
    "    # set chunk size limit\n",
    "    chunk_size_limit = 600 \n",
    "\n",
    "    # Define LLM\n",
    "    llm_predictor = LLMPredictor(llm = OpenAI(temperature = 0.5, model_name = 'text-davinci-003', max_tokens = num_outputs))\n",
    "    \n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit = chunk_size_limit)\n",
    " \n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    \n",
    "    index = GPTSimpleVectorIndex.from_documents(documents)\n",
    "\n",
    "    index.save_to_disk('index.json')\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb29f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_ai(input_text):\n",
    "    \n",
    "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
    "    \n",
    "    response = index.query(input_text, response_mode = \"compact\")\n",
    "    \n",
    "    return response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efbb0c1",
   "metadata": {},
   "source": [
    "### Interface with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73c5d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(fn = chat_ai,\n",
    "                     inputs = gr.inputs.Textbox(lines = 7, \n",
    "                     label = 'Ask me about the World Cup 2022'),\n",
    "                     outputs = gr.Textbox(lines = 7, label = \"Answer\"),\n",
    "                     theme = 'gradio/soft',\n",
    "                     title = 'Project: ChatGPT + Custom Knowledge',\n",
    "                     description = 'Enhancing the knowledge of ChatGPT by incorporating information about the World Cup 2022, that has occurred after the knowledge cutoff date of Chat GPT, which is September 2021. This was achieved by indexing data from newspapers and blogs.\\n By Jennifer Miraballes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6068d719",
   "metadata": {},
   "source": [
    "### Create Index\n",
    "\n",
    "Running this and chating with the bot will requiere credits from OpenAI account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ced273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 538601 tokens\n"
     ]
    }
   ],
   "source": [
    "# 'docs' is the folder where our train data is stored in one or more files\n",
    "index = construct_index('docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fed948",
   "metadata": {},
   "source": [
    "### Launch Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d918722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 4314 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 21 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 4580 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 16 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 4597 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 13 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 4308 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 23 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 4095 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 24 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 4150 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 26 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 4189 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 26 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 4075 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 9 tokens\n"
     ]
    }
   ],
   "source": [
    "iface.launch(share = True)"
   ]
  },
  ,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
